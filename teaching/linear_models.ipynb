{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c67a51-74c4-457b-ab40-594b97b3145d",
   "metadata": {},
   "source": [
    "# Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c07d8-895f-463b-a2d3-d1df17653c94",
   "metadata": {},
   "source": [
    "## Comparing train and test errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6832428c-911d-4626-8826-b2d0a4666828",
   "metadata": {},
   "source": [
    "see video/slides online: https://inria.github.io/scikit-learn-mooc/overfit/learning_validation_curves_slides.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50b66a-b327-49e5-ad11-5c05f4ccdc35",
   "metadata": {},
   "source": [
    "Video notes\n",
    "\n",
    "linear = linear in *parameters*, not in predictors. $\\Leftrightarrow$ linear combination of inputs\n",
    "\n",
    "Example: estimating housing prices\n",
    "- target: sale price\n",
    "- predictors: living area, year built, N full baths\n",
    "- linear approximation: explain how the paramaters are interpreted. the parameters are found automatically with the `fit` method in scikit-learn\n",
    "- slope chosen = minimize distance between prediction and the datapoints (red lines)\n",
    "- which is itself the squared error -- can compute ourselves with numpy\n",
    "- then bring this to higher dimensions\n",
    "\n",
    "For classification: logistic regression\n",
    "- binary output 0/1\n",
    "- the model learns to predict a *probability* of one class vs the other\n",
    "- in the area to the very right or left, the model is very confident about one class vs the other. in between, the model makes less certain predictions.\n",
    "- strictly speaking, the logistic regression itself is not a linear model, but the log relative probability of one vs the other class is linear in the parameters\n",
    "- in 2 dimensions: x1, x2 are input variables, colors is the outcome variable\n",
    "- the straight lines are equi-probability lines in x1-x2 space (see right-hand image: \"horizontal cuts through the shape\"\n",
    "- the line is straight because of the linear assumption (linear combination of inputs)\n",
    "\n",
    "Generalize: multiclass classification\n",
    "- model will predict probability for each of the three groups (groups are mutually exclusive)\n",
    "\n",
    "Linear models are not suited to all data\n",
    "- it's ok as long as data are approximately linearly separable -- example of inductive bias (?). in this case, the model is underfitting\n",
    "- we could try to solve it with a new feature, or with a different model\n",
    "\n",
    "Take-home messages\n",
    "- linear models are simple and fast baselines for regression and classification\n",
    "- they can underfit when number of features is way smaller than number of samples. in this case, engineering new features can help\n",
    "- when we have many features, simple linear models are hard to beat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e54855-d58f-4b11-ac45-c87c323bddb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc409e-2d57-42e4-b2dc-1e03bbf45362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
